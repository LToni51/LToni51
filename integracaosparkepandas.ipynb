{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONIMEOqkZf8bImmWVklQUk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LToni51/LToni51/blob/main/integracaosparkepandas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "UTILIZAçÃO DO PYSPARK"
      ],
      "metadata": {
        "id": "iVtm1C2heHt5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBfuIvRFd-GF",
        "outputId": "103c1614-d0d9-4fa3-d33a-857f061a994e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "f"
      ],
      "metadata": {
        "id": "QqSMtnhIeNtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "print(spark)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7hmkpofeNLx",
        "outputId": "3b9c0d3e-d8dd-4cbf-848b-edc0ca84d5bc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7caaeae3c8c0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = spark.read.csv('/content/sample_data/california_housing_test.csv', header=True, inferSchema=True )"
      ],
      "metadata": {
        "id": "R8WzSCEHerLZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXIBINDO O DATASET"
      ],
      "metadata": {
        "id": "IGWEJ7D_fcPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnCbovOZfDeh",
        "outputId": "b942dcd4-fd84-44ee-876b-2fa83ab7af54"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(longitude=-122.05, latitude=37.37, housing_median_age=27.0, total_rooms=3885.0, total_bedrooms=661.0, population=1537.0, households=606.0, median_income=6.6085, median_house_value=344700.0)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CONTANDO AS LINHAS DO DATASET"
      ],
      "metadata": {
        "id": "seJP3tkmfe1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPI8W9N_fiZh",
        "outputId": "e91892c9-a0dd-4a1d-e0c2-c8e3d1c5929a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3000"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora vamos criar uma tabela SQL temporária com os dados do “dataset”. Para isso, devemos executar o código abaixo:"
      ],
      "metadata": {
        "id": "Vq7s1vE1fqc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.createOrReplaceTempView('tabela_temporaria')\n",
        "print(spark.catalog.listTables())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2NcUS1RgFDf",
        "outputId": "8ce50951-7c31-4d46-edfc-69bf584b812b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Table(name='tabela_temporaria', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "fazendo consulta sql"
      ],
      "metadata": {
        "id": "7A9WXpNAgr__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query ='FROM tabela_temporaria SELECT longitude, latitude LIMIT 5'\n",
        "saida = spark.sql(query)\n",
        "saida.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDg5LIDlguXn",
        "outputId": "43c8851e-5b01-4352-e507-bdeb2c40480e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+\n",
            "|longitude|latitude|\n",
            "+---------+--------+\n",
            "|  -122.05|   37.37|\n",
            "|   -118.3|   34.26|\n",
            "|  -117.81|   33.78|\n",
            "|  -118.36|   33.82|\n",
            "|  -119.67|   36.33|\n",
            "+---------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A primeira parte da nossa solução vai ser dividida nos seguintes passos:\n",
        "\n",
        "\n",
        "Implementar a consulta SQL na tabela “tabela_temporaria” que já carregamos no Spark para retornar a quantidade máxima de quartos.\n",
        "Executar a consulta SQL no Spark e, assim, obter um DataFrame do Spark.\n",
        "Converter o resultado da etapa anterior para um DataFrame do Pandas.\n",
        "Imprimir o resultado da consulta.\n",
        "Converter o valor do DataFrame para um valor inteiro."
      ],
      "metadata": {
        "id": "yAT-ZK6rhoyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query1 = 'SELECT MAX(total_rooms) as maximo_quartos FROM tabela_temporaria'\n",
        "q_maximo_quartos = spark.sql(query1)\n",
        "pd_maximo_quartos = q_maximo_quartos.toPandas()\n",
        "print('a quantidade maxima de quartos é {}'.format(pd_maximo_quartos['maximo_quartos']))\n",
        "qtd_maximo_quartos = int(pd_maximo_quartos.loc[0, 'maximo_quartos'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdJ7KcZQjvm2",
        "outputId": "209b878c-324b-47e4-edf6-2effa3df7a07"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a quantidade maxima de quartos é 0    30450.0\n",
            "Name: maximo_quartos, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Devemos notar que, na primeira linha da saída, aparecem os valores “0” e “30450.0”, que são, respectivamente, a localização do elemento e o valor dentro do DataFrame. Na linha de baixo, aparece o campo “Name” com o rótulo “máximo_quartos”, nome que associamos ao retorno do SQL.\n",
        "\n",
        "\n",
        "Agora, vamos continuar a solução, que consiste em obter a localização do bloco residencial com a maior quantidade de quartos. Para isso, vamos implementar os seguintes passos:\n",
        "\n",
        "\n",
        "\n",
        "Implementar a consulta SQL para retornar a latitude e longitude da residência com a quantidade máxima de quartos que obtivemos na execução do programa anterior.\n",
        "Executar o SQL no Spark e obter o resultado no DataFrame do Spark.\n",
        "Converter o DataFrame do Spark para o DataFrame do Pandas.\n",
        "Exibir o resultado."
      ],
      "metadata": {
        "id": "v1YZCRwcneQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query2 = 'SELECT longitude, latitude FROM tabela_temporaria WHERE total_rooms ='+str(qtd_maximo_quartos)\n",
        "\n",
        "localizacao_maximo_quartos = spark.sql(query2)\n",
        "pd_localizacao_maximo_quartos = localizacao_maximo_quartos.toPandas()\n",
        "print(pd_localizacao_maximo_quartos.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8u9oKTqxngml",
        "outputId": "ed84b702-9a12-4bbe-899c-aaab8d5bb67d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   longitude  latitude\n",
            "0     -117.2     33.58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converter Pandas DataFrame para Spark DataFrame\n",
        "\n",
        "Agora, vamos estudar um exemplo que converte um DataFrame do Pandas para um DataFrame no Spark. O nosso exemplo é composto dos seguintes passos:\n",
        "\n",
        "\n",
        "\n",
        "Geração de dados aleatórios que seguem a distribuição normal com média e desvio-padrão que nós fornecemos. Usamos a biblioteca Numpy para gerar os dados e a Pandas para organizá-los em um DataFrame.\n",
        "Converter o DataFrame do Pandas para um DataFrame do Spark.\n",
        "\n",
        "Imprimir a lista de tabelas no catálogo do Spark.\n",
        "\n",
        "Adicionar a tabela temporária no catálogo do Spark.\n",
        "\n",
        "Examinar as tabelas no catálogo do Spark novamente."
      ],
      "metadata": {
        "id": "LlqNakUfpLB9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c1cywzCorxYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "media = 0\n",
        "desvio_padrao = 0.1\n",
        "pd_temporario = pd.DataFrame(np.random.normal(media,desvio_padrao,100))\n",
        "spark_temporario = spark.createDataFrame(pd_temporario)\n",
        "print(spark.catalog.listTables())\n",
        "spark_temporario.createOrReplaceTempView('nova_tabela_temporaria')\n",
        "print(spark.catalog.listTables())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQxictoepRZ2",
        "outputId": "401b4658-1a5d-451f-cedf-18fbd19ac3b0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Table(name='tabela_temporaria', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]\n",
            "[Table(name='nova_tabela_temporaria', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True), Table(name='tabela_temporaria', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop"
      ],
      "metadata": {
        "id": "PWPABWa1r3SX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}